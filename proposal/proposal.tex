\PassOptionsToPackage{usenames}{color}
\documentclass[11pt]{article}
\usepackage{etex} % remove ``No room for a new \dimen'' error

\usepackage{relsize} % relative font sizes (e.g. \smaller). must precede ACL style
\usepackage{acl2014}
\usepackage[linkcolor=blue]{hyperref}
\usepackage{natbib}
\newcommand{\citeposs}[1]{\citeauthor{#1}'s (\citeyear{#1})}

%\usepackage{times}
%\usepackage{latexsym}


\usepackage[boxed]{algorithm2e}
\renewcommand\AlCapFnt{\small}
\usepackage[small,bf,skip=5pt]{caption}
\usepackage{sidecap} % side captions
\usepackage{rotating}	% sideways

% Italicize subparagraph headings
\usepackage{titlesec}
\titleformat*{\subparagraph}{\itshape}
\titlespacing{\subparagraph}{%
  1em}{%              left margin
  0pt}{% space before (vertical)
  1em}%               space after (horizontal)

% Numbered Examples and lists
\usepackage{lingmacros}

\usepackage{enumitem} % customizable lists
\setitemize{noitemsep,topsep=0em,leftmargin=*}
\setenumerate{noitemsep,leftmargin=0em,itemindent=13pt,topsep=0em}


\usepackage{textcomp}
% \usepackage{arabtex} % must go after xparse, if xparse is used!
%\usepackage{utf8}
% \setcode{utf8} % use UTF-8 Arabic
% \newcommand{\Ar}[1]{\RL{\novocalize #1}} % Arabic text

\usepackage{listings}

\lstset{
  basicstyle=\itshape,
  xleftmargin=3em,
  aboveskip=0pt,
  belowskip=-3pt, %-.5\baselineskip, % correct for extra paragraph break inserted after listing
  literate={->}{$\rightarrow$}{2}
           {α}{$\alpha$}{1}
           {δ}{$\delta$}{1}
           {(}{$($}{1}
           {)}{$)$}{1}
           {[}{$[$}{1}
           {]}{$]$}{1}
           {|}{$|$}{1}
           {+}{\ensuremath{^+}}{1}
           {*}{\ensuremath{^*}}{1}
}

\usepackage{amssymb}	%amsfonts,eucal,amsbsy,amsthm,amsopn
\usepackage{amsmath}

\usepackage{mathptmx}	% txfonts
\usepackage[scaled=.8]{beramono}
\usepackage[T1]{fontenc}
\usepackage[utf8x]{inputenc}

\usepackage{MnSymbol}	% must be after mathptmx

\usepackage{latexsym}

% Graphics
\usepackage{tikz}
\usetikzlibrary{arrows,positioning,calc} 



% Tables
\usepackage{array}
\usepackage{multirow}
\usepackage{booktabs} % pretty tables
\usepackage{multicol}
\usepackage{footnote}
\newcolumntype{H}{>{\setbox0=\hbox\bgroup}c<{\egroup}@{}} % hidden column

\usepackage{url}
\usepackage[usenames]{color}
\usepackage{xcolor}

% colored frame box
\newcommand{\cfbox}[2]{%
    \colorlet{currentcolor}{.}%
    {\color{#1}%
    \fbox{\color{currentcolor}#2}}%
}

\usepackage[normalem]{ulem} % \uline
\usepackage{colortbl}
\usepackage{graphicx}
\usepackage{subcaption}
%\usepackage{tikz-dependency}
%\usepackage{tikz}
%\usepackage{tree-dvips}
%\usetikzlibrary{arrows,positioning,calc} 
\usepackage{xytree}

\usepackage{xspace} % \xspace command for macros (inserts a space unless followed by punctuation)

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\setlength\titlebox{6.5cm}    % Expanding the titlebox



% Author comments
\usepackage{color}
\newcommand\bmmax{0} % magic to avoid 'too many math alphabets' error
\usepackage{bm}
\definecolor{orange}{rgb}{1,0.5,0}
\definecolor{mdgreen}{rgb}{0,0.6,0}
\definecolor{mdblue}{rgb}{0,0,0.7}
\definecolor{dkblue}{rgb}{0,0,0.5}
\definecolor{dkgray}{rgb}{0.3,0.3,0.3}
\definecolor{slate}{rgb}{0.25,0.25,0.4}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{ltgray}{rgb}{0.7,0.7,0.7}
\definecolor{purple}{rgb}{0.7,0,1.0}
\definecolor{lavender}{rgb}{0.65,0.55,1.0}

% Settings for algorithm listings
% \lstset{
%   language=Python,
%   upquote=true,
%   showstringspaces=false,
%   formfeed=\newpage,
%   tabsize=1,
%   commentstyle=\itshape\color{lavender},
%   basicstyle=\small\smaller\ttfamily,
%   morekeywords={lambda},
%   emph={upward,downward,tc},
%   emphstyle=\underbar,
%   aboveskip=0cm,
%   belowskip=-.5cm
% }
%\renewcommand{\lstlistingname}{Algorithm}


\newcommand{\ensuretext}[1]{#1}
\newcommand{\cjdmarker}{\ensuretext{\textcolor{green}{\ensuremath{^{\textsc{CJ}}_{\textsc{D}}}}}}
\newcommand{\nssmarker}{\ensuretext{\textcolor{magenta}{\ensuremath{^{\textsc{NS}}_{\textsc{S}}}}}}
\newcommand{\nasmarker}{\ensuretext{\textcolor{red}{\ensuremath{^{\textsc{NA}}_{\textsc{S}}}}}}
\newcommand{\lkmarker}{\ensuretext{\textcolor{blue}{\ensuremath{^{\textsc{L}}_{\textsc{K}}}}}}
\newcommand{\swswmarker}{\ensuretext{\textcolor{orange}{\ensuremath{^{\textsc{S}}_{\textsc{S}}}}}}
\newcommand{\abmarker}{\ensuretext{\textcolor{purple}{\ensuremath{^{\textsc{A}}_{\textsc{B}}}}}}
\newcommand{\arkcomment}[3]{\ensuretext{\textcolor{#3}{[#1 #2]}}}
%\newcommand{\arkcomment}[3]{}
\newcommand{\nss}[1]{\arkcomment{\nssmarker}{#1}{magenta}}
\newcommand{\aj}[1]{\arkcomment{\cjdmarker}{#1}{green}}
\newcommand{\dirk}[1]{\arkcomment{\nasmarker}{#1}{red}}
\newcommand{\lk}[1]{\arkcomment{\lkmarker}{#1}{blue}}
\newcommand{\swsw}[1]{\arkcomment{\swswmarker}{#1}{orange}}
\newcommand{\ab}[1]{\arkcomment{\abmarker}{#1}{purple}}
\newcommand{\wts}{\mathbf{w}}
\newcommand{\g}{\mathbf{g}}
\newcommand{\f}{\mathbf{f}}
\newcommand{\x}{\mathbf{x}}
\newcommand{\y}{\mathbf{y}}
\newcommand{\overbar}[1]{\mkern 1.5mu\overline{\mkern-1.5mu#1\mkern-1.5mu}\mkern 1.5mu} % \bar is too narrow in math
\newcommand{\cost}{c}

\usepackage{nameref}
\usepackage{cleveref}

% use \S for all references to all kinds of sections, and \P to paragraphs
% (sadly, we cannot use the simpler \crefname{} macro because it would insert a space after the symbol)
\crefformat{part}{\S#2#1#3}
\crefformat{chapter}{\S#2#1#3}
\crefformat{section}{\S#2#1#3}
\crefformat{subsection}{\S#2#1#3}
\crefformat{subsubsection}{\S#2#1#3}
\crefformat{paragraph}{\P#2#1#3}
\crefformat{subparagraph}{\P#2#1#3}
%\crefmultiformat{part}{\S#2#1#3}{ and~\S#2#1#3}{, \S#2#1#3}{, and~\S#2#1#3}
%\crefmultiformat{chapter}{\S#2#1#3}{ and~\S#2#1#3}{, \S#2#1#3}{, and~\S#2#1#3}
\crefmultiformat{section}{\S#2#1#3}{ and~\S#2#1#3}{, \S#2#1#3}{, and~\S#2#1#3}
\crefmultiformat{subsection}{\S#2#1#3}{ and~\S#2#1#3}{, \S#2#1#3}{, and~\S#2#1#3}
\crefmultiformat{subsubsection}{\S#2#1#3}{ and~\S#2#1#3}{, \S#2#1#3}{, and~\S#2#1#3}
\crefmultiformat{paragraph}{\P\P#2#1#3}{ and~#2#1#3}{, #2#1#3}{, and~#2#1#3}
\crefmultiformat{subparagraph}{\P\P#2#1#3}{ and~#2#1#3}{, #2#1#3}{, and~#2#1#3}
%\crefrangeformat{part}{\mbox{\S\S#3#1#4--#5#2#6}}
%\crefrangeformat{chapter}{\mbox{\S\S#3#1#4--#5#2#6}}
\crefrangeformat{section}{\mbox{\S\S#3#1#4--#5#2#6}}
\crefrangeformat{subsection}{\mbox{\S\S#3#1#4--#5#2#6}}
\crefrangeformat{subsubsection}{\mbox{\S\S#3#1#4--#5#2#6}}
\crefrangeformat{paragraph}{\mbox{\P\P#3#1#4--#5#2#6}}
\crefrangeformat{subparagraph}{\mbox{\P\P#3#1#4--#5#2#6}}
% for \label[appsec]{...}
\crefname{part}{Part}{Parts}
\Crefname{part}{Part}{Parts}
\crefname{chapter}{ch.}{ch.}
\Crefname{chapter}{Ch.}{Ch.}
\crefname{figure}{figure}{figures}
\crefname{appsec}{appendix}{appendices}
\Crefname{appsec}{Appendix}{Appendices}
\crefname{algocf}{algorithm}{algorithms}
\Crefname{algocf}{Algorithm}{Algorithms}
\crefname{enums,enumsi}{example}{examples}
\Crefname{enums,enumsi}{Example}{Examples}
\crefname{}{example}{examples} % lingmacros \toplabel has no internal name for the kind of label
\Crefname{}{Example}{Examples}
\crefformat{enums}{(#2#1#3)}
\crefformat{enumsi}{(#2#1#3)}
\crefformat{}{(#2#1#3)}
\crefrangeformat{enums}{\mbox{(#3#1#4--#5#2#6)}}
\crefrangeformat{enumsi}{\mbox{(#3#1#4--#5#2#6)}}

\ifx\creflastconjunction\undefined%
\newcommand{\creflastconjunction}{, and\nobreakspace} % Oxford comma for lists
\else%
\renewcommand{\creflastconjunction}{, and\nobreakspace} % Oxford comma for lists
\fi%

\newcommand*{\Fullref}[1]{\hyperref[{#1}]{\Cref*{#1}: \nameref*{#1}}}
\newcommand*{\fullref}[1]{\hyperref[{#1}]{\cref*{#1}: \nameref{#1}}}
\newcommand{\fnref}[1]{footnote~\ref{#1}} % don't use \cref{} due to bug in (now out-of-date) cleveref package w.r.t. footnotes
\newcommand{\Fnref}[1]{Footnote~\ref{#1}}


% Space savers
% From http://www.eng.cam.ac.uk/help/tpl/textprocessing/squeeze.html
% \addtolength{\dbltextfloatsep}{-.5cm} % space between last top float or first bottom float and the text.
% \addtolength{\intextsep}{-.5cm} % space left on top and bottom of an in-text float.
% \addtolength{\abovedisplayskip}{-.5cm} % space before maths
% \addtolength{\belowdisplayskip}{-.5cm} % space after maths
% %\addtolength{\topsep}{-.5cm} %space between first item and preceding paragraph
% \setlength{\belowcaptionskip}{-.25cm}


% customize \paragraph spacing
% \makeatletter
% \renewcommand{\paragraph}{%
%   \@startsection{paragraph}{4}%
%   {\z@}{.2ex \@plus 1ex \@minus .2ex}{-1em}%
%   {\normalfont\normalsize\bfseries}%
% }
% \makeatother


% Special macros
\newcommand{\tg}[1]{\texttt{#1}}	% tag name
\newcommand{\sst}[1]{\textsc{#1}} % supersense category
\newcommand{\gfl}[1]{%\renewcommand\texttildelow{{\lower.74ex\hbox{\texttt{\char`\~}}}} % http://latex.knobs-dials.com/
\mbox{\textsmaller{\texttt{#1}}}}	% supersense tag symbol
%\newcommand{\lex}[1]{\textsmaller{\textsf{\textcolor{slate}{\textbf{#1}}}}}	% example lexical item 
\newcommand{\tagdef}[1]{#1\hfill} % tag definition
\newcommand{\tagt}[2]{\ensuremath{\underset{\textrm{\textlarger{\tg{#2}}}\strut}{\w{#1}\rule[-.3\baselineskip]{0pt}{0pt}}}} % tag text (a word or phrase) with an SST. (second arg is the tag)
\newcommand{\tagtt}[3]{\begin{tabular}{@{\hspace{2pt}}c@{\hspace{2pt}}} \texttt{#2}\\ #1 \\ \texttt{#3}\end{tabular}}
\newcommand{\tagts}[3]{\begin{tabular}{@{\hspace{2pt}}c@{\hspace{2pt}}} \texttt{#2\vphantom{\textlarger{Ĩ}}}\\ #1 \\ \sst{#3}\end{tabular}}
\newcommand{\tgt}[2]{\ensuremath{\underset{\text{\textlarger{#2}\strut}}{\text{#1\rule[-.3\baselineskip]{0pt}{\baselineskip}}}}} % tag text (a word or phrase, not underlined) with a label. (second arg is the label)
\newcommand{\glosst}[2]{\ensuremath{\underset{\textrm{#2}}{\textrm{#1}}}} % gloss text (a word or phrase) (second arg is the gloss)
\newcommand{\AnnA}[0]{\mbox{\textbf{Ann-A}}} % annotator A
\newcommand{\AnnB}[0]{\mbox{\textbf{Ann-B}}} % annotator B
\newcommand{\sys}[1]{\mbox{\textbf{#1}}}   % name of a system (one of our experimental conditions)
\newcommand{\dataset}[1]{\mbox{\textsc{#1}}}	% one of the datasets in our experiments
\newcommand{\datasplit}[1]{\mbox{\textbf{#1}}}	% portion one of the datasets in our experiments

\newcommand{\w}[1]{\textit{#1}}	% word
\newcommand{\lex}[1]{\textit{#1}} % lexical item
\newcommand{\tweet}[1]{\textsf{#1}}	% tweet
\newcommand{\twbank}[0]{\textsc{Tweebank}\xspace}
\newcommand{\foster}[0]{\textsc{Foster}\xspace}
\newcommand{\twparser}[0]{\textsc{Tweeboparser}\xspace}
\newcommand{\tat}[0]{\textasciitilde}
\newcommand{\backtick}[0]{\textasciigrave}

%\newcommand{\finalversion}[1]{#1}
\newcommand{\finalversion}[1]{}
\newcommand{\shortversion}[1]{#1}
\newcommand{\considercutting}[1]{#1}
\newcommand{\longversion}[1]{} % ...if only there were more space...

\hyphenation{WordNet}
\hyphenation{WordNets}
\hyphenation{VerbNet}
\hyphenation{FrameNet}
\hyphenation{SemCor}
\hyphenation{PennConverter}
\hyphenation{TurboParser}
\hyphenation{Tweebo-parser}
\hyphenation{Twee-bank}
\hyphenation{an-aly-sis}
\hyphenation{an-aly-ses}
\hyphenation{news-text}
\hyphenation{base-line}
\hyphenation{de-ve-lop-ed}
\hyphenation{comb-over}

\title{Joint Tagging of Multiword Expressions and Supersenses}

% Lingpeng Kong, Nathan Schneider, Swabha Swayamdipta, Archna Bhatia, Chris Dyer, and Noah A. Smith 
\author{
Nathan Schneider \\
		School of Informatics\\
	   	University of Edinburgh\\
	    Edinburgh, UK\\
	    {\tt nschneid@inf.ed.ac.uk} \And
Dirk Hovy \quad Anders Johannsen\\
Center for Language Technology\\
University of Cophenhagen\\
Copenhagen, Denmark\\
{\tt dirk@cst.dk}\\ {\tt ajohannsen@hum.ku.dk} \And
Marine Carpuat\\
National Research Council\\
Ottawa, Ontario, Canada\\
{\tt marine.carpuat@nrc.gc.ca}}


\date{}

\begin{document}
\maketitle
\begin{abstract}
We propose that the CoNLL~2015 Shared Task 
consist of analyzing the lexical semantics of sentences 
in a broad-coverage fashion. 
The task formulation consists of two steps which can be performed jointly:
(a)~chunking tokens within sentences into \textbf{multiword expressions} (MWEs), and 
(b)~assigning coarse \textbf{supersense} labels to all noun and verb expressions. 
The evaluation will build upon recently annotated datasets in two social web genres.
We expect that the task formulation will foster engagement across 
multiple subcommunities of computational semantics, 
and facilitate empirical comparison of methodologically diverse systems.

\nss{Note: this document uses natbib rather than the standard ACL bib macros: 
According to \citet{baldwin-10}, statistical association measures 
are one technique for extracting MWE types \citep[cf.][\emph{inter alia}]{pecina-10}.}
\end{abstract}

\section{Introduction}

Broad-coverage NLP techniques have, thanks in part to previous CoNLL shared tasks, 
become mainstream for several aspects of language---including morphology, syntax, 
and some areas of semantics (notably, named entity recognition \nss{2002/2003}, semantic role labeling \nss{2004/2005/2008/2009}, 
and coreference resolution \nss{2011/2012}).
Explicit analysis of lexical semantics, by contrast, has been more difficult to scale 
to broad coverage.\nss{need to say something about purely unsupervised distributional approaches?} 
This might be attributable to the dominant paradigm of word sense disambiguation 
with lexical resources such as WordNet \citep{wordnet}.
The fine-grained sense inventories in these resources 
are difficult to annotate in corpora, result in considerable data sparseness, 
and do not readily generalize to out-of-vocabulary words.
The main corpus that is annotated with WordNet senses, 
SemCor \citep{semcor}, reflects several genres of edited text; 
unfortunately, expanding the annotations to new genres, such as social web text 
or transcribed speech, would be quite expensive.
This limits the applicability of SemCor-based NLP tools 
and restricts opportunities for linguistic studies of lexical semantics in corpora.

Here we propose a linguistic representation, annotated data, and evaluation measures 
aimed at establishing a broad-coverage paradigm for lexical semantic analysis, 
one which we believe is suitable for a CoNLL shared task. 
We motivate the two components of the lexical representation---multiword expressions 
and supersenses---before delving into the data and evaluation.

\nss{motivation: lexical semantic analysis of MWEs and supersenses in English. 
semantics is a hot topic.
highlights: (a) broad-coverage---not too far from NER; hopefully avoid some of the limitations of traditional WSD. 
(b) robust to domain---in fact, evaluation consists of 2 social web domains.
(c) potential to benefit from many different NLP techniques, including sequence tagging/chunking, 
parsing, distributional word representations, language models, 
use of language resources such as WordNet, (other buzzwords?)}

\paragraph{Multiword expressions.} 
For the most part, contemporary approaches to English syntactic and semantic analysis
treat space-separated words as the basic units of structure. 
%It has even been suggested that dependency syntax, for example, 
%offers a good approximation of semantic structure.
But for sentences with noncompositional or idiosyncratic expressions, this fails to reflect 
the basic units of meaning. For example, consider:
\enumsentence{The staff leaves a lot to be desired .}
\enumsentence{I googled restaurants in the area and Fuji Sushi came up and reviews were great so I made a carry out order of : L 17 .}
In these sentences, \lex{a lot}, \lex{leaves\ldots to be desired}, 
\lex{Fuji Sushi}, \lex{came up}, \lex{made\ldots order}, and \lex{carry out}
are all \textbf{multiword expressions} (MWEs) 
in that their combined meanings can be thought of as ``prepackaged'' 
in a single lexical expression that happens to be written with spaces.\footnote{This is not to say that MWEs cannot have ``normal'' 
syntactic structure; some are more fixed in their form while others are quite flexible. 
For instance, \lex{make\ldots order} can be passivized just like canonical transitive verb phrases.}
MWEs such as these (and many other flavors) have attracted a great deal of attention 
within computational semantics; see \citet{baldwin-10} for a review.
\nss{identification}

\paragraph{Supersenses.}
\dirk{stab at it...}
The fundamental problem with any sort of semantic analysis of text its the granularity of the chosen representation. Ideally, one would like to have both broad word coverage \emph{and} sufficient distinction. A representation represents good coverage if it provides labels for most word classes. It offers sufficient distinction if it allows to group words into clusters, in order to make generalizations about semantic classes. The latter is typically anti-correlated with the number of available labels.

Popular representations are named-entity tags and WordNet \cite{fellbaum-90} word senses. However, both present a problem in one of the desired aspects. Named entity tags have a reasonable distinction, and are thus frequently used to find entities of a certain type. However, as the name already suggests, this representation is limited to (proper) nouns, and has thus poor coverage. 
Word senses, on the other hand, cover all content word classes. However, they are lemma-specific (i.e., sense number two of \emph{house} and \emph{building} are not the same), and do not allow for generalization to larger groups.
\dirk{Mention work on other languages?}

Supersenses are also based off of WordNet, but represent a higher level in the taxonomy. They originated from a higher-level clustering of nouns and verbs into groups, and thus have good coverage.\footnote{Supersenses for adjectives and adverbs exist, but are based on morpho-syntactic rather than semantic properties. There is, however, recent work on developing a taxonomy for adjective senses, see \cite{tsvetkov-14}.}
Since they group individual word senses into larger groups, they allow for distinction beyond the lemma level, i.e., they assign the same supersense to words with different word senses.

To date, many of the analysis problems in computational lexical semantics are niche topics. 
By integrating broad-coverage MWE identification together with supersense tagging 
as a single analysis task, we hope to encourage the comparison of systems on benchmark datasets 
and to foster cross-pollination between different subcommunities. 

\nss{encourage creativity of solutions}
Semantic analysis is a powerful and promising topic, but it has been largely confined to newswire, i.e., standardized text written mostly in English. By including a relatively new and heterogeneous domain like Twitter in the data, we want to encourage creative approaches that go beyond the tried and true newswire-trained approach, and open the community to wider applications. 

\nss{related work: these representations build on ones that have been explored, 
but go a step beyond. (want to convince reader that there will be interest in the task based on previous results, 
but also that it is somewhat novel.)}

\nss{relevance to study of human language learning}

The proposed task will be limited to English. 
This reflects what we belive is realistic given existing resources 
and the organizers' ability to annotate new data. 
If the task is successful, we would advocate a second incarnation 
adding evaluations in other languages\footnote{These might extend existing supersense datasets in other languages 
to encode MWEs. We are aware of supersense annotations in Italian, Arabic, and Chinese \nss{cites}.} and/or child-directed speech.

\section{Representation}

\begin{figure}\centering\small
%I highly~recommend_ Garage_Pros _to my friends .  
%"labels": {"8": ["friends", "GROUP"], "3": ["recommend", "communication"], "4": ["Garage", "GROUP"]}, "_": [[3, 6], [4, 5]], "words": [["I", "PRP"], ["highly", "RB"], ["recommend", "VBP"], ["Garage", "NNP"], ["Pros", "NNPS"], ["to", "IN"], ["my", "PRP$"], ["friends", "NNS"], [".", "."]], "~": [[2, 3, 6]]}
\begin{tikzpicture}[baseline={($(current bounding box.center)+(0,-0.5ex)$)}, node distance=0cm, auto,]
 \node[inner sep=0cm]                                  (n1) {\tagts{I}{O}{}};
 \node[inner sep=0cm,above right=0pt of n1.south east] (n2) {\tagts{highly}{B}{}};
 \node[inner sep=0cm,above right=0pt of n2.south east] (n3) {\tagts{recommend}{\color{red}Ĩ}{\color{red}\textsc{v:comm.}}};
 \node[inner sep=0cm,above right=0pt of n3.south east] (n4) {\tagts{Garage}{\color{blue}b}{\color{blue}\textsc{n:group}}};
 \node[inner sep=0cm,above right=0pt of n4.south east] (n5) {\tagts{Pros}{\color{blue}ī}{}};
 \node[inner sep=0cm,above right=0pt of n5.south east] (n6) {\tagts{to}{\color{red}Ī}{}};
\path[-,thick,dotted, bend left=45] (n2.north) edge (n3.north); 
\path[-,thick,red, bend left=45] (n3.north) edge (n6.north);
 \path[-,thick,blue, bend left=45] (n4.north) edge (n5.north);
 \node[inner sep=0cm,above right=0pt of n6.south east] (n7) {\tagts{my}{O}{}};
 \node[inner sep=0cm,above right=0pt of n7.south east] (n8) {\tagts{friends}{O}{\textsc{n:group}}};
 \node[inner sep=0cm,above right=0pt of n8.south east] (n9) {\tagts{.}{O}{}};
\end{tikzpicture}
\caption{Illustration of the target representation: MWE positional tags are shown above the sentence 
and noun and verb supersenses are shown below the sentence. 
Links illustrate the behavior of the MWE tags: 
solid links for strong MWEs and dotted links for weak MWEs. 
The supersense labeling must respect the strong MWEs; thus, \sst{v:comm(unication)} 
and the first \sst{n:group} each apply to two-word units---\emph{to} and \emph{Pros} 
must not receive separate supersenses.}
\label{fig:ex1}
\end{figure}

% ewtb.r.055976.1 The staff leaves_ a_lot _to_be_desired .
% ewtb.r.389298.4 I highly~recommend_ Garage_Pros _to my friends .  
%"labels": {"8": ["friends", "GROUP"], "3": ["recommend", "communication"], "4": ["Garage", "GROUP"]}, "_": [[3, 6], [4, 5]], "words": [["I", "PRP"], ["highly", "RB"], ["recommend", "VBP"], ["Garage", "NNP"], ["Pros", "NNPS"], ["to", "IN"], ["my", "PRP$"], ["friends", "NNS"], [".", "."]], "~": [[2, 3, 6]]}
% ewtb.r.329991.1 I googled restaurants in the area and Fuji_Sushi came_up and reviews were great so I made_ a carry_out _order of : L 17 .       {"labels": {"2": ["googled", "communication"], "3": ["restaurants", "GROUP"], "6": ["area", "LOCATION"], "8": ["Fuji", "GROUP"], "10": ["came", "communication"], "13": ["reviews", "COMMUNICATION"], "14": ["were", "stative"], "18": ["made", "communication"], "20": ["carry", "possession"]}, "_": [[8, 9], [10, 11], [18, 22], [20, 21]], "words": [["I", "PRP"], ["googled", "VBD"], ["restaurants", "NNS"], ["in", "IN"], ["the", "DT"], ["area", "NN"], ["and", "CC"], ["Fuji", "NNP"], ["Sushi", "NNP"], ["came", "VBD"], ["up", "RB"], ["and", "CC"], ["reviews", "NNS"], ["were", "VBD"], ["great", "JJ"], ["so", "RB"], ["I", "PRP"], ["made", "VBD"], ["a", "DT"], ["carry", "VB"], ["out", "RP"], ["order", "NN"], ["of", "IN"], [":", ":"], ["L", "$"], ["17", "CD"], [".", "."]], "~": []}




\nss{given a sentence, what should be predicted. extra examples in appendices are encouraged}

\nss{file format with example. UTF-8}

\section{Data}

\begin{table}[ht]
\begin{center}
\resizebox{ \columnwidth }{!}{
\begin{tabular}{lll}
n.Tops	&	n.object	&	v.cognition\\
n.act	&	n.person	&	v.communication\\
n.animal	&	n.phenomenon	&	v.competition\\
n.artifact	&	n.plant	&	v.consumption\\
n.attribute	&	n.possession	&	v.contact\\
n.body	&	n.process	&	v.creation\\
n.cognition	&	n.quantity	&	v.emotion\\
n.communication	&	n.relation	&	v.motion\\
n.event	&	n.shape	&	v.perception\\
n.feeling	&	n.state	&	v.possession\\
n.food	&	n.substance	&	v.social\\
n.group	&	n.time	&	v.stative\\
n.location	&	v.body	&	v.weather\\
n.motive	&	v.change	&	
\end{tabular}
}% resize
\end{center}
\captionof{table}{The 41 noun and verb supersenses in WordNet\label{tab:supersenses}}
\end{table}%


The task will use two existing datasets for both training and evaluation. 
These capture two genres of social web text:
\begin{itemize}
\item \textsc{Reviews}: This 55,000-token portion of the English Web Treebank \citep[EWTB;][]{ewtb} 
consists of 723~online user reviews for services such as restaurants and beauticians.\footnote{Each review is a separate document; 
no metadata about the reviews is available. EWTB provides sentence segmentation, word tokenization, and a Penn Treebank--style 
syntactic parse of each sentence.}
It was recently manually annotated in full, first for heterogeneous MWEs \citep[released as the CMWE Corpus\footnote{\url{http://www.ark.cs.cmu.edu/LexSem/}}]{schneider-14-corpus}, 
and then---building on the MWE annotations---for noun and verb supersenses (as yet unreleased; for nouns, the conventions of \citealp{schneider-12} were followed). 
The annotations consist of approximately 3,500~MWEs, 9,000~noun expressions, 8,000~main verb expressions, and 1,000~auxiliary verbs; 
see \cref{tbl:reviews} for a breakdown.
\nss{example. strong vs. weak distinction?}


\item \textsc{Tweets}: Two samples of Twitter messages were recently annotated with supersenses \citep{johannsen-14}: 
(a) supersense annotations for the POS+NER-annotated data set described in \citep{Ritter:ea:11}, which we use for training, development and evaluation, using the splits proposed in \citep{Derczynski:ea:13}, and (b) supersense annotations for a sample of 200 tweets, which we use for additional, out-of-sample evaluation. %We call these data sets {\sc Ritter}-$\{${\sc Train,Dev,Eval}$\}$ and {\sc In-House-Eval}, respectively. 
The latter was downloaded in 2013 and was previously used (with part-of-speech annotation) in \citep{Plank:ea:14}.

For both data sets,  annotators were shown pre-annotations from a heuristic supersense tagging system (based on the most frequent sense of each word)
and asked to correct the boundaries and supersense labels. The supersenses are annotated with spans defined by the BIO (Begin-Inside-Other) notation, there was no explicit MWE annotation phase, 
though many of the multiword chunks tagged with a noun or verb supersense would be considered MWEs. 

The average raw agreement on the data from Ritter was 0.86 (Cohen's $\kappa$ 0.77). The majority of tokens received the \emph{O} label by all annotators; this happended in 515 out of 841 cases. Excluding these instances to evaluate the performance on the more difficult content words, raw agreement dropped to 0.69 and Cohen's $\kappa$ to 0.69.
The second data had an average raw agreement of 0.65 and their Cohen's $\kappa$ 0.62. 
%For this data set, we also compute $F_1$, defined as usual as the harmonic mean of recall and precision. To compute this, we set one of the annotators as gold data and the other as predicted data. However, since $F_{1}$ is symmetrical, the order does not matter. The annotation $F_{1}$ gives us another estimate of annotation difficulty. We present the figures in Table \ref{table:properties-of-dataset}.


\nss{example. number of single-word, multiword SSTs grouped by noun, verb.}
\nss{splits.}
\end{itemize}

\nss{what new (or revised) annotations are needed? 
(CPH should have resources to annotate a new Twitter test set with supersenses. possibly MWEs as well)
(participants should be banned from using the CMWE data lest they train on the test set.)
(is it OK if, e.g., supersense tagging conventions are different in different datasets?)
be as specific as possible about timeline}

\begin{table*}\small\centering
\begin{tabular}{@{}>{\textsc\bgroup}l<{\egroup}@{~~}r@{~~}r@{~~}c@{~~}|l@{~~}r@{~~}c@{~~}>{\smaller}Hl@{~~}r@{~~}c@{~~}>{\smaller}H@{}}
\multicolumn{4}{c}{\textbf{MWE}} & \multicolumn{4}{c}{\textbf{Noun}} & \multicolumn{4}{c}{\textbf{Verb}} \\
\cmidrule(r){1-4}\cmidrule(lr){5-8}\cmidrule(l){9-12}

% adv\_prep &                       25 &      1 & \lex{even~though}
%27 & V\_A &                       19 &      6 & \lex{make~sure} \\
%28 & V\_P\_N &                     14 &      6 & \lex{put~at~ease} \\
%29 & \#\_N &                       20 &      \phantom{0}& 5~star:~9\quad 2~star:~2\quad 800~number:~1\quad one~bit:~1\quad ten~star:~1\quad 360~restraunt:~1\quad \\
%30 & N\_A &                       18 &      \phantom{0}& year~old:~9\quad month~old:~3\quad years~old:~2\quad cost~effective:~1\quad lightning~fast:~1\quad \\


n\_n &                      331 &      1 & \lex{customer~service}
	& \sst{group} & 1469 & \lex{place} & 6 &         \sst{stative} & 2922 & \lex{is} & 1 \\
pn\_pn &                      325 &      1 & \lex{santa~fe} 
	& \sst{person} & 1202 & \lex{people} & 1 &        \sst{cognition} & 1093 & \lex{know} & 4 \\
v\_prep &                      217 &     44 & \lex{work~with}
	& \sst{artifact} & 971 & \lex{car} & 2 &      \sst{communication} & 974 & \lex{recommend} & 2 \\
v\_part &                      149 &     42 & \lex{pick~up} 
	& \sst{cognition} & 771 & \lex{way} & 4 &    \sst{social} & 944 & \lex{use} & 5 \\
v\_n &                       31 &    107 & \lex{take~time}
	& \sst{food} & 766 & \lex{food} & 21 &         \sst{motion} & 602 & \lex{go} & 6 \\
adj\_n &                      133 &      3 & \lex{front~desk}
	& \sst{act} & 700 & \lex{service} & 3 &          \sst{possession} & 309 & \lex{pay} & 7 \\
v\_adv &                      103 &     30 & \lex{come~in}
	& \sst{location} & 638 & \lex{area} & 8 &     \sst{change} & 274 & \lex{fix} & 3 \\
det\_n &                       83 &      1 & \lex{a~lot}
	& \sst{time} & 530 & \lex{day} & 9 &         \sst{emotion} & 249 & \lex{love} & 11 \\
prep\_n &                       67 &      8 & \lex{on~time}
	& \sst{event} & 431 & \lex{experience} & 14 &       \sst{perception} & 143 & \lex{see} & 9 \\
adv\_adv &                       72 &      1 & \lex{at~least}
	& \sst{communication} & 417 & \lex{review} & 5 &   \sst{consumption} & 93 & \lex{have} & 12 \\
v\_det\_n &                     46 &     21 & \lex{take~the~time}
	& \sst{possession} & 339 & \lex{price} & 16 &   \sst{body} & 82 & \lex{get\ldots done} & 13 \\
v\tat n &           7 &     56 & \lex{do~job}
	& \sst{attribute} & 205 & \lex{quality} & 7 &     \sst{creation} & 64 & \lex{cook} & 10 \\
pn\_pn\_pn &             63 &     \phantom{0}& \lex{home~delivery~service}
	& \sst{quantity} & 102 & \lex{amount} & 13 &      \sst{contact} & 46 & \lex{put} & 8 \\
adv\tat v &                     49 &      \phantom{0}& \it \lex{highly~recommend}
	& \sst{animal} & 88 & \lex{dog} & 18 &         \sst{competition} & 11 & \lex{win} & 14 \\
prep\_det\_n &                     33 &      6 & \lex{over~the~phone}
	& \sst{body} & 87 & \lex{hair} & 11 &          \sst{weather} & 0 & --- & 15 \\
\cline{9-12}
adj\_prep &                       39 &      \phantom{0}& \lex{pleased~with}
	& \sst{state} & 56 & \lex{pain}            & 10 & \textbf{\smaller[.5] all 15 VSSTs} & \textbf{7806} & \\
prep\_prep &                       39 &      \phantom{0}& \lex{out~of}
	& \sst{natural object} & 54 & \lex{flower} & 15 & \textsmaller[.5]{auxiliary verbs} & 1191 & \lex{have} &  \\
v\_pron &                       38 &      \phantom{0}& \lex{thank~you}
	& \sst{relation} & 35 & \lex{portion} & 19 &  \\
v\_v &                        8 &     30 & \lex{get~do}
	& \sst{substance} & 34 & \lex{oil} & 12     &  \\
n\tat n &                    34 &      1 & \lex{channel~guide}
	& \sst{feeling} & 34 & \lex{discomfort} & 20 &  \\
adj\tat n &                    31 &      \phantom{0}& \lex{hidden~gem}
	& \sst{process} & 28 & \lex{process} & 22 &     \\
v\_n\_prep &                     16 &     15 & \lex{take~care~of}
	& \sst{motive} & 25 & \lex{reason} & 25 & \\
n\_v &                       18 &     10 & \lex{mind~blow}
	& \sst{phenomenon} & 23 & \lex{result} & 17 & \\
pn\_poss &                       28 &      \phantom{0}& \lex{bj~s}
	& \sst{shape} & 6 & \lex{square} & 24 & \\
det\_adj &                       28 &      \phantom{0}& \lex{a~few}
	 & \sst{plant} & 5 & \lex{tree} & 23 & \\
\multicolumn{4}{c|}{\textsmaller{\ldots~435 additional patterns~\ldots}} % 460-25
	& \sst{other} & 2 & \lex{stuff} & 26 & \\
\cline{1-4}\cline{5-8}
\multicolumn{1}{@{}l}{\textbf{\smaller[.5] all MWEs}} & \textbf{2948} & \textbf{535} & \textbf{= 3483 total instances}
 & \textbf{\smaller[.5] all 26 NSSTs} & \textbf{9018} 
\end{tabular}

\caption{Top MWE patterns along with frequency-ranked noun and verb supersense tagsets from the \textsc{Reviews} corpus. 
The last column of each entry shows the most frequent lexical item (lemma sequence) of the class.
MWE patterns are strong (\_) or weak (\tat) sequences of coarse parts of speech. (\textsc{pn} stands for ``proper noun.'')
Contiguous and gappy MWE counts are distinguished: e.g.,~78\% of instances of the \textsc{v\_n} pattern 
contain a gap (with one or more intervening words).
Supersense counts do not distinguish single-word vs.~multiword expressions.
}
\label{tbl:reviews}
\end{table*}

\nss{issue to resolve: licensing of Web Treebank source text}

\section{Evaluation}

\subsection{System Submission Process}

On May~5,~2015, teams will be furnished with the test data (minus the gold labels). 
They will have until May~10 to submit up to 3~system predictions for evaluation. 
The test data will include sentences from both evaluation domains, 
in a random order: to encourage robust systems, 
the domain of each sentence will not be marked at test time, 
and the proportion of sentences from each domain is not guaranteed to be 
the same in the trial, train, dev, and test sets.\nss{is this crazy?}

\subsection{Splits}

We adopt \citeposs{schneider-14} splits of the \textsc{Reviews} data: 
101~documents (500~sentences, 7,171~words) are held out as a test set, 
leaving 3,312~sentences\slash 48,408~words for training\slash development.

\nss{Tweets}

\subsection{Inputs}

\nss{open, closed track. if a team submits systems to the closed track and fewer than 3 systems to the open track, 
they will be encouraged to submit the closed track results to the open track as well.}
The main goal of the task is to find new creative ways to improve the performance of semantic annotation on various domains. Participants are thus encouraged to make wide use of available resources, and the use of external resources for distant or direct supervision is explicitly allowed. Theses entries will be judged in an \emph{open} track.

However, we also offer a \emph{closed} track, whose participants can only use the provided resources. We envision this track for teams that are more interested in the algorithmic problem of the task, and who do not want to spend too much time on the development and incorporation of other data sources. In order not to disadvantage these groups, they will be judged separately.

\nss{systems must respect the input tokenization}
Due to the nature of the task, which tries to capture MWEs, participants of both tracks are required to respect the input tokenization of the train, development, and test data.

\nss{domain label at training time, but not test time. no other metadata}

\nss{a few possibilities for tracks---
\begin{description}
\item[closed track] provided training data; we'll allow WordNet and provide (auto?)~POS, Brown clusters as well? 
no other resources allowed
\item[semi-open track] we provide a large unlabeled corpus, in addition to closed track resources
\item[unsupervised track] no token-level MWE or SST labels may be used
\item[open track] any data/resources (except the CMWE corpus, if that includes the test set) may be used
\end{description}
I feel like there are pluses and minuses to all 4 of these tracks. maybe we should just mention the possibilities 
and survey teams to ensure we only evaluate tracks that generate sufficient interest}

\subsection{Measures}

 
We have written scripts that compute three kinds of scores: 
these compare a system's predictions against the gold standard 
with respect to MWEs, supersenses, and their combination.


\begin{figure*}\centering %\small
% {"34": ["price", "POSSESSION"], "36": ["means", "cognition"], "26": ["was", "stative"], "29": ["budge", "change"]}
\begin{tikzpicture}[baseline={($(current bounding box.center)+(0,-0.5ex)$)}, node distance=0cm, auto,]
 \node[inner sep=0cm] (n1) {\tagtt{he}{O}{O}\tagtt{was}{O}{O}\tagtt{willing}{O}{O}\tagtt{to}{O}{O}};
 \node[inner sep=0cm,above right=0pt of n1.south east] (n2) {\tagtt{budge}{\color{red}B}{O}};
 \node[inner sep=0cm,above right=0pt of n2.south east] (n3) {\tagtt{a}{\color{blue}b}{B}};
 \node[inner sep=0cm,above right=0pt of n3.south east] (n4) {\tagtt{little}{\color{blue}ī}{Ī}};
 \node[inner sep=0cm,above right=0pt of n4.south east] (n5) {\tagtt{on}{\color{red}Ī}{O}};
 \node[inner sep=0cm,above right=0pt of n5.south east] (n6) {\tagtt{the}{O}{O}\tagtt{price}{O}{O}};
 \path[-,thick,red, bend left=45] (n2.north) edge (n5.north);
 \path[-,thick,blue, bend left=45] (n3.north) edge (n4.north);
\path[-,thick, bend right=45] (n3.south) edge (n4.south);
 \node[inner sep=0cm,above right=0pt of n6.south east] (n7) {\tagtt{which}{O}{O}};
 \node[inner sep=0cm,above right=0pt of n7.south east] (n8) {\tagtt{means}{B}{O}};
 \node[inner sep=0cm,above right=0pt of n8.south east] (n9) {\tagtt{a}{\color{orange}Ĩ}{B}};
 \node[inner sep=0cm,above right=0pt of n9.south east] (n10) {\tagtt{lot}{\color{orange}Ī}{Ĩ}};
  \path[-,thick,orange, bend left=45] (n9.north) edge (n10.north);
\path[-,thick,dotted, bend right=45] (n9.south) edge (n10.south);
 \node[inner sep=0cm,above right=0pt of n10.south east] (n11) {\tagtt{to}{Ĩ}{o}};
 \node[inner sep=0cm,above right=0pt of n11.south east] (n12) {\tagtt{me}{Ĩ}{Ĩ}};
  \path[-,thick,dotted, bend left=45] (n8.north) edge (n9.north); 
\path[-,thick,dotted, bend right=45] (n10.south) edge (n12.south); 
  \path[-,thick,dotted, bend left=45] (n10.north) edge (n11.north);
  \path[-,thick,dotted, bend left=45] (n11.north) edge (n12.north);
 \node[inner sep=0cm,above right=0pt of n12.south east] (n13) {\tagtt{.}{O}{O}};
\end{tikzpicture}
\caption{A \textsc{Reviews} sentence with two hypothetical MWE annotations. 
Strong links are depicted with solid arcs, and weak links with dotted arcs.\nss{relationship to tags format} 
Precision of the top annotation relative to the bottom one is $1/3$ with weak links removed 
and $2/6$ with weak links strengthened to strong links (note that a link between words $w_1$ and $w_2$ is ``matched'' 
if, in the other annotation, there is a path between $w_1$ and $w_2$). 
The respective recall values are $1/1$ and $3/3$. 
Overall $F_1$ is computed as the average of two $F_1$-scores, i.e.~$\tfrac{1}{3} \cdot \tfrac{1}{1}/(\tfrac{1}{3} + \tfrac{1}{1}) + \tfrac{2}{6}\cdot \tfrac{3}{3}/(\tfrac{2}{6}+\tfrac{3}{3}) = 0.50$.}
\label{fig:linkmeasure}
\end{figure*}

The \textbf{MWE} measure looks at precision, recall, and $F_1$ 
of the identified MWEs. Tokens not involved in a predicted or gold MWE
do not factor into this measure. 
To award partial credit for cases of partial overlap between a predicted MWE 
and a gold MWE, these scores are computed based on \emph{links} between 
consecutive (but not necessarily adjacent) tokens in an expression \citep{schneider-14}.
\Cref{fig:linkmeasure} illustrates the links implied by two different annotations 
of a sentence. The precision is the proportion of predicted links whose words 
both belong to the same expression in the gold standard. 
Recall is the same as precision, but swapping the predicted and gold annotations.\footnote{This computation on the basis of links 
is a slight simplification of the MUC coreference measure \citep{vilain-95}.}
A weak link is treated as intermediate between a strong link and no link at all: 
precision, recall, and $F_1$ computed on strong links only are averaged 
with the respective calculations computed on all links without regard to strength. 
These calculations are demonstrated in the caption of \cref{fig:linkmeasure} for the example.

To isolate the \textbf{supersense} classification performance, 
we simply compute accuracy of the supersense part of the full tag.\nss{for all tags, or all non-strong-I tags, or just ones with actual supersenses?} 
Because the supersense of a strong MWE is marked on its first word, 
this will have the effect of penalizing errors on the left boundary of an MWE, 
but it is less sensitive to the MWE analysis than the link-based measure. 

The third measure is a hybrid of the first two: it computes \textbf{overall} performance 
in terms of link-based $F_1$, where for each supersense label, 
a self-link encoding that label is added to the first word of the expression.
Credit is only given for a self-link if in the other analysis, 
a corresponding self-link occurs with the same supersense.

All of these measures are microaverages, computed globally over tokens in the corpus 
(rather than normalized within each sentence and averaged across sentences).

\nss{other possibilities that we should probably not mention in the interest of simplicity: 
weight MWE links according to supersense match;
full tag accuracy; 
giving less credit for supersense mismatches that disagree on POS}

\subsection{Conditions}

All measures will be reported separately for each of the two evaluation domains, 
and in a combined figure with the two domains weighted equally. 

Competitive rankings will be determined from the joint MWE+SST measure in the combination of the domains. 
There will be two such rankings: one in the closed track, and one in the open track.
\nss{At the discretion of the organizers, honorable mention awards may go to systems that place well on 
any of the measures, or that were especially creative in their approach?}

\subsection{Baselines}

\nss{we already have systems to serve as baselines, each customized to one domain. 
we can even show results with current versions of data! \cref{tbl:baselines}}

Two open source supersense tagging systems are maintained by authors of this proposal; 
these can serve as reference implementations for the task and baselines for the evaluation. 
\textbf{AMALGrAM}\footnote{\uline{A} \uline{M}achine \uline{A}nalyzer of \uline{L}exical \uline{Gr}oupings \uline{A}nd \uline{M}eanings}
extends the discriminative MWE identification model of \citet{schneider-14}\footnote{Code and MWE-only model/training data at \url{http://www.ark.cs.cmu.edu/LexSem/}} 
to also label supersenses. 
It is trained on the \textsc{Reviews} data: this learning is fully supervised 
(though the model includes unsupervised word cluster features). 
\textbf{Copenhagen} \nss{\ldots}.

\Cref{tbl:baselines} reports preliminary results on the current versions of the two evaluation datasets, 
using the measures described above (precision and recall are omitted for brevity).
\nss{discussion? presumably: we see that the domain gap is huge---neither system is robust to the two domains.
this in one of the challenges that systems in our task will have to face.}

\begin{table*}\centering\small
\begin{tabular}{lccccccc}
           &  & \multicolumn{3}{c}{\textsc{Reviews}} & \multicolumn{3}{c}{\textsc{Tweets}} \\
           \cmidrule(r){3-5}\cmidrule(l){6-8}
\bfseries System     & \bfseries Train Domain & \bfseries MWE & \bfseries SST & \bfseries Full
                                              & \bfseries ``MWE'' & \bfseries SST & \bfseries Full \\
\midrule
AMALGrAM (supervised 1st-order discriminative with gappy MWEs) & \textsc{Reviews} & \# & \# & \# & \# & \# & \# \\
Copenhagen (\nss{semi-supervised 2nd-order discriminative?}) & \textsc{Tweets} & \# & \# & \#  & \# & \# & \# \\
\end{tabular}
\caption{Evaluation of baseline systems. For \textsc{Tweets}, ``MWE'' is placed in scare quotes 
because we have not yet systematically annotated the data for MWEs, so this preliminary evaluation 
is against supersense annotators' chunking decisions for noun and verb expressions.}
\label{tbl:baselines}
\end{table*}

\section{Conclusion}

\nss{TODO}

\bibliographystyle{aclnat}
% you bib file should really go here
\setlength{\bibsep}{10pt}
{\fontsize{10}{12.25}\selectfont
\bibliography{proposal}}



\end{document}
